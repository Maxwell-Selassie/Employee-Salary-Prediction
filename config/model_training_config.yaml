# ==============================
# MODEL TRAINING CONFIGURATION
# ==============================
# Production-grade ML training pipeline with MLflow tracking

# ==============================
# PROJECT METADATA
# ==============================
project:
  name: 'Maxwell Selassie Hiamatsu'
  version: 1.0
  description: 'Employee Salary Prediction - Model Training'

# ==============================
# DATA PATHS
# ==============================
data:
  train_data: 'data/processed/train_set.csv'
  val_data: 'data/processed/dev_set.csv'
  test_data: 'data/processed/test_set.csv'
  target_column: 'current_salary_log'
  
  # Preprocessing artifacts
  scaler_path: 'models/scaler.pkl'
  encoder_path: 'models/encoder.pkl'

# ==============================
# MLFLOW CONFIGURATION
# ==============================
mlflow:
  tracking_uri: sqlite:///mlruns/employee_salary_prediction.db
  experiment_name: 'Employee_Salary_Prediction - Model_Training'
  run_name_pattern: 'run_{model_name}_{timestamp}'
  
  # What to track
  track:
    hyperparameters: True
    metrics: True
    model_artifacts: True
    feature_importance: True
    plots: True
    training_duration: True
    dataset_metadata: True
    preprocessing_artifacts: True
  
  # Model Registry
  model_registry:
    enabled: True
    register_best_model: True
    model_name: 'Employee_Salary_Prediction_Model'
    
    # Compare with production
    compare_with_production: True
    min_improvement_required: 0.01


# logging configuration
logging:
  log_dir: 'logs/'
  log_level: 'INFO'
  max_bytes: 10485760
  backup_count: 7
  format: '%(asctime)s -%(name) %(levelname)s -%(funcName)s:%(lineno)d - %(message)s'
  date_format: '%Y-%m-%d %H:%M:%S'

model_validation:
  enabled: True
  
  thresholds:
    mean_squared_error:
      max: 0.10
      warning: 0.12
      target: 0.08
    
    r2_score:
      min: 0.80
      warning: 0.80
      target: 0.90
    
    train_test_gap:
      max: 0.15
      warning: 0.10



# ==============================
# MODELS CONFIGURATION
# ==============================
models:
  # Baseline Model
  baseline:
    name: 'Ridge'
    enabled: True
    params:
      random_state: 42
      max_iter: 1000

  
  # Tree-Based Models
  tree_based:
    RandomForest:
      enabled: True
      params:
        random_state: 42
        class_weight: 'balanced'
        n_estimators: 100
        n_jobs: -1
    
    XGBoost:
      enabled: True
      params:
        random_state: 42
        n_estimators: 100
        learning_rate: 0.1
        n_jobs: -1

    
    LightGBM:
      enabled: True
      params:
        random_state: 42
        n_estimators: 100
        learning_rate: 0.1
        n_jobs: -1
        verbose: -1

# ==============================
# CROSS-VALIDATION
# ==============================
cross_validation:
  enabled: True
  method: 'kfold'
  n_splits: 10
  shuffle: True
  random_state: 42
  
  # Report CV scores
  report_cv_scores: True
  aggregate_method: 'mean'  # mean, median

# ==============================
# HYPERPARAMETER TUNING
# ==============================
hyperparameter_tuning:
  enabled: True
  method: 'optuna'
  tune_top_n_models: 3
  n_trials: 50
  cv_splits: 5  # Use 5-fold CV during tuning (faster)
  timeout: 3600  # 1 hour timeout per model
  
  # Optuna settings
  optuna:
    direction: 'minimize'  # minimize MSE
    sampler: 'TPESampler'  # Tree-structured Parzen Estimator
    pruner: 'MedianPruner'  # Early stopping for bad trials
    n_jobs: 1  # Sequential for stability
  
  # Hyperparameter search spaces
  search_spaces:
    Ridge:
      C: ['log_uniform', 0.001, 100]
      penalty: ['categorical', ['l1', 'l2']]
      solver: ['categorical', ['auto', 'svd']]
    
    RandomForest:
      n_estimators: ['int', 50, 500]
      max_depth: ['int', 3, 30]
      min_samples_split: ['int', 2, 20]
      min_samples_leaf: ['int', 1, 10]
      max_features: ['categorical', ['sqrt', 'log2']]
    
    XGBoost:
      n_estimators: ['int', 50, 500]
      max_depth: ['int', 3, 15]
      learning_rate: ['log_uniform', 0.001, 0.3]
      subsample: ['uniform', 0.6, 1.0]
      colsample_bytree: ['uniform', 0.6, 1.0]
      gamma: ['uniform', 0, 5]
      min_child_weight: ['int', 1, 10]
    
    LightGBM:
      n_estimators: ['int', 50, 500]
      max_depth: ['int', 3, 15]
      learning_rate: ['log_uniform', 0.001, 0.3]
      num_leaves: ['int', 20, 150]
      min_child_samples: ['int', 5, 100]
      subsample: ['uniform', 0.6, 1.0]
      colsample_bytree: ['uniform', 0.6, 1.0]

# ==============================
# EVALUATION METRICS
# ==============================
metrics:
  primary_metric: 'mean_squared_error'

  regression_metrics:
    - 'mean_absolute_error'
    - 'root_mean_squared_error'
    - 'r2_score'
    - 'mean_squared_error'
    - 'median_absolute_error'
    - 'mean_absolute_percentage_error'  
  


# ==============================
# FEATURE IMPORTANCE
# ==============================
feature_importance:
  enabled: True
  
  methods:
    native:
      enabled: True
      applicable_to: ['RandomForest', 'XGBoost', 'LightGBM']
    
    permutation:
      enabled: True
      n_repeats: 10
      random_state: 42
      applicable_to: 'all'
    
    shap:
      enabled: False  # Disabled for speed (enable for deep analysis)
      applicable_to: 'all'
  
  # Plotting
  plot:
    enabled: True
    top_n_features: 20
    save_format: 'png'
    dpi: 300

# ==============================
# MODEL SELECTION
# ==============================
model_selection:
  # Automatic selection
  auto_select:
    enabled: True
    criteria: 'primary_metric'  # primary_metric, balanced_score
    select_best_from: 'all_trained_models'  # all_trained_models, tuned_models_only

# ==============================
# MODEL PERSISTENCE
# ==============================
model_persistence:
  save_best_model_only: True
  best_model_path: 'models/best_model.joblib'
  
  # Save additional artifacts
  save_artifacts:
    feature_names: True
    metrics_summary: True
    training_history: True
  
  artifacts_path: 'artifacts/'

# ==============================
# REPRODUCIBILITY
# ==============================
reproducibility:
  random_state: 42
  set_seeds:
    numpy: True
    sklearn: True
    xgboost: True
    lightgbm: True
    optuna: True

  
  track_during_training:
    - 'training_time'
    - 'memory_usage'
    - 'data_shapes'
    - 'class_distribution'
    - 'feature_count'
    - 'convergence_warnings'

# ==============================
# PLOTTING
# ==============================
plotting:
  enabled: True
  plots_dir: 'artifacts/plots/'
  
  plots_to_generate:
    - 'feature_importance'
    - 'learning_curve'
  
  plot_format: 'png'
  dpi: 300
  style: 'seaborn-v0_8-darkgrid'

# ==============================
# ERROR HANDLING
# ==============================
error_handling:
  on_model_failure: 'stop'  # stop, skip, retry
  max_retries: 0
  save_failed_models: False

# ==============================
# NOTES
# ==============================
notes:
  - 'Models trained on preprocessed and feature-engineered data'
  - 'Hyperparameter tuning uses Optuna with TPE sampler'
  - 'All experiments tracked in MLflow'
  - 'Best model registered in MLflow Model Registry as Production'

assumptions:
  - 'Data is already preprocessed and split (train/val/test)'
  - 'All features are numeric after preprocessing'
