{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e24e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow \n",
    "from pathlib import Path\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a59299e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev dataframe successfully loaded with 7500 rows and 45 features\n",
      "Train dataframe successfully loaded with 35000 rows and 44 features\n"
     ]
    }
   ],
   "source": [
    "# load processed train and dev data for modelling\n",
    "def load_data(filepath: str, dataset_type: str) -> pd.DataFrame:\n",
    "    '''\n",
    "        Loads processed data from csv source\n",
    "    \n",
    "        This function loads processed data from csv file\n",
    "        that will be used for modelling\n",
    "        \n",
    "        Args:\n",
    "            filepath: path to processed dataset\n",
    "            dataset_type: whether the dataset is train, dev or test dataframe\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: returns a dataframe containing processed data\n",
    "            \n",
    "        Examples:\n",
    "            >>> df = load_data('data/processed/train_set.csv')\n",
    "                df.head()\n",
    "    '''\n",
    "    filename = Path(filepath)\n",
    "    if not filename.exists():\n",
    "        raise FileNotFoundError(f'File not found! Check filepath and try again later!')\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # check that the df is not empty\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(f'Dataframe cannot be empty!')\n",
    "    \n",
    "    print(f'{dataset_type} dataframe successfully loaded with {df.shape[0]} rows and {df.shape[1]} features')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "TRAIN_DATA_PATH = '../data/processed/train_set.csv'\n",
    "DEV_DATA_PATH = '../data/processed/dev_set.csv'\n",
    "\n",
    "dev_df = load_data(DEV_DATA_PATH, 'Dev')\n",
    "train_df = load_data(TRAIN_DATA_PATH, 'Train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a08d622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_cols = [col for col in dev_df if col not in train_df]\n",
    "missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e73832bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The dataset must not contain null values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll columns in the dataset must be numerical\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAll validation passed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mdata_quality_checks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m data_quality_checks(dev_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mdata_quality_checks\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     16\u001b[39m missing = df.isnull().sum()\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe dataset must not contain null values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m n_duplicates = df[df.duplicated()].sum()\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_duplicates != \u001b[32m0\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: The dataset must not contain null values"
     ]
    }
   ],
   "source": [
    "# perform one last data quality check before modelling\n",
    "def data_quality_checks(df: pd.DataFrame) -> None:\n",
    "    '''\n",
    "        Performs one last data quality check before modelling\n",
    "\n",
    "        Args:\n",
    "            df: pandas' dataframe to be validated\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: if any of the validations fail\n",
    "\n",
    "        Example:\n",
    "            >>> data_quality_checks(df)\n",
    "    '''\n",
    "\n",
    "    missing = df.isnull().sum()\n",
    "    if not len(missing) == 0:\n",
    "        raise ValueError(\"The dataset must not contain null values\")\n",
    "    \n",
    "    n_duplicates = df[df.duplicated()].sum()\n",
    "    if n_duplicates != 0:\n",
    "        raise ValueError(\"The dataset must not contain duplicate rows\")\n",
    "\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numerical_cols) != len(df.columns):\n",
    "        raise ValueError(\"All columns in the dataset must be numerical\")\n",
    "\n",
    "    print(\"All validation passed!\")\n",
    "\n",
    "data_quality_checks(train_df)\n",
    "data_quality_checks(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16257222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9abb3f21-75a4-4298-8955-68071132df77",
       "rows": [
        [
         "Unnamed: 0",
         "0"
        ],
        [
         "Employee_age",
         "0"
        ],
        [
         "Current_Salary",
         "0"
        ],
        [
         "Number_of_Children",
         "0"
        ],
        [
         "years_experience",
         "0"
        ],
        [
         "past_projects",
         "0"
        ],
        [
         "current_projects",
         "0"
        ],
        [
         "performance_rating",
         "0"
        ],
        [
         "Job_Satisfaction",
         "0"
        ],
        [
         "Work_Life_Balance",
         "0"
        ],
        [
         "is_outlier",
         "0"
        ],
        [
         "Maritial_Status_True",
         "0"
        ],
        [
         "Divorced_earlier_Yes",
         "0"
        ],
        [
         "Father_alive_Yes",
         "0"
        ],
        [
         "Mother_alive_Yes",
         "0"
        ],
        [
         "Education_level_Diploma",
         "0"
        ],
        [
         "Education_level_High School",
         "0"
        ],
        [
         "Education_level_Master's",
         "0"
        ],
        [
         "Education_level_PhD",
         "0"
        ],
        [
         "Department_Finance",
         "0"
        ],
        [
         "Department_HR",
         "0"
        ],
        [
         "Department_Operations",
         "0"
        ],
        [
         "Department_R&D",
         "0"
        ],
        [
         "Department_Sales",
         "0"
        ],
        [
         "Department_Support",
         "0"
        ],
        [
         "Role_Analyst",
         "0"
        ],
        [
         "Role_Customer Specialist",
         "0"
        ],
        [
         "Role_DevOps Engineer",
         "0"
        ],
        [
         "Role_HR Executive",
         "0"
        ],
        [
         "Role_HR Manager",
         "0"
        ],
        [
         "Role_ML Engineer",
         "0"
        ],
        [
         "Role_Operations Coordinator",
         "0"
        ],
        [
         "Role_Ops Manager",
         "0"
        ],
        [
         "Role_Researcher",
         "0"
        ],
        [
         "Role_Sales Manager",
         "0"
        ],
        [
         "Role_Scientist",
         "0"
        ],
        [
         "Role_Senior Analyst",
         "0"
        ],
        [
         "Role_Software Engineer",
         "0"
        ],
        [
         "Role_Support Engineer",
         "0"
        ],
        [
         "Employee_age_log",
         "0"
        ],
        [
         "past_projects_log",
         "0"
        ],
        [
         "years_experience_log",
         "0"
        ],
        [
         "Job_Satisfaction_log",
         "0"
        ],
        [
         "Work_Life_Balance_log",
         "0"
        ],
        [
         "Current_Salary_log",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 45
       }
      },
      "text/plain": [
       "Unnamed: 0                     0\n",
       "Employee_age                   0\n",
       "Current_Salary                 0\n",
       "Number_of_Children             0\n",
       "years_experience               0\n",
       "past_projects                  0\n",
       "current_projects               0\n",
       "performance_rating             0\n",
       "Job_Satisfaction               0\n",
       "Work_Life_Balance              0\n",
       "is_outlier                     0\n",
       "Maritial_Status_True           0\n",
       "Divorced_earlier_Yes           0\n",
       "Father_alive_Yes               0\n",
       "Mother_alive_Yes               0\n",
       "Education_level_Diploma        0\n",
       "Education_level_High School    0\n",
       "Education_level_Master's       0\n",
       "Education_level_PhD            0\n",
       "Department_Finance             0\n",
       "Department_HR                  0\n",
       "Department_Operations          0\n",
       "Department_R&D                 0\n",
       "Department_Sales               0\n",
       "Department_Support             0\n",
       "Role_Analyst                   0\n",
       "Role_Customer Specialist       0\n",
       "Role_DevOps Engineer           0\n",
       "Role_HR Executive              0\n",
       "Role_HR Manager                0\n",
       "Role_ML Engineer               0\n",
       "Role_Operations Coordinator    0\n",
       "Role_Ops Manager               0\n",
       "Role_Researcher                0\n",
       "Role_Sales Manager             0\n",
       "Role_Scientist                 0\n",
       "Role_Senior Analyst            0\n",
       "Role_Software Engineer         0\n",
       "Role_Support Engineer          0\n",
       "Employee_age_log               0\n",
       "past_projects_log              0\n",
       "years_experience_log           0\n",
       "Job_Satisfaction_log           0\n",
       "Work_Life_Balance_log          0\n",
       "Current_Salary_log             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data further into features and target set\n",
    "def features_target_split(df: pd.DataFrame, target: str = 'Current_Salary') -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "        Split the dataset further into features and target splits\n",
    "        \n",
    "        This function takes the given dataframe and splits it\n",
    "        into the feature set and target set for modelling purposes\n",
    "    \n",
    "        Args:\n",
    "            df: pd.DataFrame = Provided dataframe\n",
    "            target: str = Target variable in the given dataframe\n",
    "\n",
    "        Returns:\n",
    "            A tuple of the features and target sets\n",
    "\n",
    "        Examples:\n",
    "            >>> x, y = features_target_split(df, 'Employee_salary')\n",
    "                x.head()\n",
    "                y.head()\n",
    "    '''\n",
    "    y = df[target].copy()\n",
    "    if not isinstance(y, pd.Series):\n",
    "        raise ValueError('The target variable must be a pandas series')\n",
    "    \n",
    "    if len(y) == 0:\n",
    "        raise ValueError(f'The target variable cannot be empty')\n",
    "    \n",
    "    x = df.drop(columns=[target]).copy()\n",
    "    if not isinstance(x, pd.DataFrame):\n",
    "        raise ValueError('The feature set must be a pandas dataframe')\n",
    "    \n",
    "    if len(x) == 0:\n",
    "        raise ValueError(f'The target variable cannot be empty')\n",
    "    \n",
    "    if len(y) != len(x):\n",
    "        raise ValueError('The length of the target variable must be equal to the length of the feature set')\n",
    "\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = features_target_split(train_df)\n",
    "x_dev, y_dev = features_target_split(dev_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
