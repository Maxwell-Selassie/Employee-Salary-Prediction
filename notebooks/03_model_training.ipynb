{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e24e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow \n",
    "from pathlib import Path\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a59299e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev dataframe successfully loaded with 7500 rows and 44 features\n",
      "Train dataframe successfully loaded with 35000 rows and 44 features\n"
     ]
    }
   ],
   "source": [
    "# load processed train and dev data for modelling\n",
    "def load_data(filepath: str, dataset_type: str) -> pd.DataFrame:\n",
    "    '''\n",
    "        Loads processed data from csv source\n",
    "    \n",
    "        This function loads processed data from csv file\n",
    "        that will be used for modelling\n",
    "        \n",
    "        Args:\n",
    "            filepath: path to processed dataset\n",
    "            dataset_type: whether the dataset is train, dev or test dataframe\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: returns a dataframe containing processed data\n",
    "            \n",
    "        Examples:\n",
    "            >>> df = load_data('data/processed/train_set.csv')\n",
    "                df.head()\n",
    "    '''\n",
    "    filename = Path(filepath)\n",
    "    if not filename.exists():\n",
    "        raise FileNotFoundError(f'File not found! Check filepath and try again later!')\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # check that the df is not empty\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(f'Dataframe cannot be empty!')\n",
    "    \n",
    "    print(f'{dataset_type} dataframe successfully loaded with {df.shape[0]} rows and {df.shape[1]} features')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "TRAIN_DATA_PATH = '../data/processed/train_set.csv'\n",
    "DEV_DATA_PATH = '../data/processed/dev_set.csv'\n",
    "\n",
    "dev_df = load_data(DEV_DATA_PATH, 'Dev')\n",
    "train_df = load_data(TRAIN_DATA_PATH, 'Train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73832bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All validation passed!\n",
      "All validation passed!\n"
     ]
    }
   ],
   "source": [
    "# perform one last data quality check before modelling\n",
    "def data_quality_checks(df: pd.DataFrame) -> None:\n",
    "    '''\n",
    "        Performs one last data quality check before modelling\n",
    "\n",
    "        Args:\n",
    "            df: pandas' dataframe to be validated\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: if any of the validations fail\n",
    "\n",
    "        Example:\n",
    "            >>> data_quality_checks(df)\n",
    "    '''\n",
    "\n",
    "    missing = df.isnull().sum().sum()\n",
    "    if missing:\n",
    "        raise ValueError(\"The dataset must not contain null values\")\n",
    "    \n",
    "    n_duplicates = df.duplicated().sum()\n",
    "    if n_duplicates != 0:\n",
    "        raise ValueError(\"The dataset must not contain duplicate rows\")\n",
    "\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numerical_cols) != len(df.columns):\n",
    "        raise ValueError(\"All columns in the dataset must be numerical\")\n",
    "\n",
    "    print(\"All validation passed!\")\n",
    "\n",
    "data_quality_checks(train_df)\n",
    "data_quality_checks(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062b545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data further into features and target set\n",
    "def features_target_split(df: pd.DataFrame, target: str = 'Current_Salary') -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "        Split the dataset further into features and target splits\n",
    "        \n",
    "        This function takes the given dataframe and splits it\n",
    "        into the feature set and target set for modelling purposes\n",
    "    \n",
    "        Args:\n",
    "            df: pd.DataFrame = Provided dataframe\n",
    "            target: str = Target variable in the given dataframe\n",
    "\n",
    "        Returns:\n",
    "            A tuple of the features and target sets\n",
    "\n",
    "        Examples:\n",
    "            >>> x, y = features_target_split(df, 'Employee_salary')\n",
    "                x.head()\n",
    "                y.head()\n",
    "    '''\n",
    "    y = df[target].copy()\n",
    "    if not isinstance(y, pd.Series):\n",
    "        raise ValueError('The target variable must be a pandas series')\n",
    "    \n",
    "    if len(y) == 0:\n",
    "        raise ValueError(f'The target variable cannot be empty')\n",
    "    \n",
    "    x = df.drop(columns=[target]).copy()\n",
    "    if not isinstance(x, pd.DataFrame):\n",
    "        raise ValueError('The feature set must be a pandas dataframe')\n",
    "    \n",
    "    if len(x) == 0:\n",
    "        raise ValueError(f'The target variable cannot be empty')\n",
    "    \n",
    "    if len(y) != len(x):\n",
    "        raise ValueError('The length of the target variable must be equal to the length of the feature set')\n",
    "\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = features_target_split(train_df)\n",
    "x_dev, y_dev = features_target_split(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00989889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (35000, 43)\n",
      "x_dev: (7500, 43)\n",
      "y_train: (35000,)\n",
      "y_dev: (7500,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train: ', x_train.shape)\n",
    "print(f'x_dev: {x_dev.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "print(f'y_dev: {y_dev.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c3641b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, learning_curve\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,root_mean_squared_error,r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ebe138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline models - performing modelling with minimal feature engineering\n",
    "models = {\n",
    "    'Ridge' : Ridge(random_state=42),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=200, max_depth=6, n_jobs=-1, random_state=1),\n",
    "    'XGBoost': XGBRegressor(),\n",
    "    'LightGBM': LGBMRegressor(random_state=2)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a974fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: Ridge...\n",
      "Model_name_train : Ridge\n",
      "Mean_squared_error_train : 177006236.93275234\n",
      "Root_mean_squared_error_train : 13304.369091871751\n",
      "Mean_absolute_error_train : 5950.650044137561\n",
      "Time_elapsed_train : 0.04302263259887695\n",
      "==================================================\n",
      "Training model: RandomForest...\n",
      "Model_name_train : RandomForest\n",
      "Mean_squared_error_train : 374677.1392871908\n",
      "Root_mean_squared_error_train : 612.1087642626846\n",
      "Mean_absolute_error_train : 299.84564094744934\n",
      "Time_elapsed_train : 39.75776767730713\n",
      "==================================================\n",
      "Training model: XGBoost...\n",
      "Model_name_train : XGBoost\n",
      "Mean_squared_error_train : 31568.296875\n",
      "Root_mean_squared_error_train : 177.67469787597656\n",
      "Mean_absolute_error_train : 100.116455078125\n",
      "Time_elapsed_train : 1.7549805641174316\n",
      "==================================================\n",
      "Training model: LightGBM...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1583\n",
      "[LightGBM] [Info] Number of data points in the train set: 35000, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 58335.359571\n",
      "Model_name_train : LightGBM\n",
      "Mean_squared_error_train : 15300556.94997373\n",
      "Root_mean_squared_error_train : 3911.592635995437\n",
      "Mean_absolute_error_train : 291.44503629623364\n",
      "Time_elapsed_train : 6.062285423278809\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f'Training model: {model_name}...')\n",
    "    start_time = time.time()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "\n",
    "    preds_train = model.predict(x_train)\n",
    "    # preds_dev = model.predict(x_dev)\n",
    "\n",
    "    results = {\n",
    "        'Model_name' : model_name,\n",
    "        'Mean_squared_error' : mean_squared_error(y_train, preds_train),\n",
    "        'Root_mean_squared_error' : root_mean_squared_error(y_train, preds_train),\n",
    "        'Mean_absolute_error' : mean_absolute_error(y_train, preds_train),\n",
    "        'Time_elapsed' : time_elapsed\n",
    "    }\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        print(f'{name}_train : {result}')\n",
    "        \n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f4522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name_train : Ridge\n",
      "cv_score_across_fold_train : [13214.01516076 11546.08185647 14546.91316514 12515.08319543\n",
      " 14545.629717  ]\n",
      "cv_score_mean_train : 13273.544618960095\n",
      "cv_score_std_train : 1166.4135659553087\n",
      "==================================================\n",
      "model_name_train : RandomForest\n",
      "cv_score_across_fold_train : [631.03277737 819.79787692 757.55650098 574.76559407 682.24520977]\n",
      "cv_score_mean_train : 693.0795918224054\n",
      "cv_score_std_train : 87.38442501704962\n",
      "==================================================\n",
      "model_name_train : XGBoost\n",
      "cv_score_across_fold_train : [9402.90039062 6235.70605469 8811.60839844 9500.25976562 9101.83203125]\n",
      "cv_score_mean_train : 8610.461328125\n",
      "cv_score_std_train : 1211.6987123585957\n",
      "==================================================\n",
      "model_name_train : LightGBM\n",
      "cv_score_across_fold_train : [7905.2099425  6079.50476953 7564.06289146 5803.99762713 7063.48003909]\n",
      "cv_score_mean_train : 6883.251053943197\n",
      "cv_score_std_train : 818.6798231414747\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# cross validation score \n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=9999)\n",
    "\n",
    "for name, model in models.items():\n",
    "    cross_val = cross_val_score(model, x_train, y_train, cv=cv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "    cv_score = -cross_val\n",
    "    results = {\n",
    "        'model_name' : name,\n",
    "        'cv_score_across_fold' : cv_score,\n",
    "        'cv_score_mean' : cv_score.mean(),\n",
    "        'cv_score_std' : cv_score.std()\n",
    "    }\n",
    "\n",
    "    for name, result in results.items():\n",
    "        print(f'{name}_train : {result}')\n",
    "\n",
    "    print('='*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
