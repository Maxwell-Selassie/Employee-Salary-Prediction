{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e24e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow \n",
    "from pathlib import Path\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59299e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev dataframe successfully loaded with 7500 rows and 44 features\n",
      "Train dataframe successfully loaded with 35000 rows and 44 features\n"
     ]
    }
   ],
   "source": [
    "# load processed train and dev data for modelling\n",
    "def load_data(filepath: str, dataset_type: str) -> pd.DataFrame:\n",
    "    '''\n",
    "        Loads processed data from csv source\n",
    "    \n",
    "        This function loads processed data from csv file\n",
    "        that will be used for modelling\n",
    "        \n",
    "        Args:\n",
    "            filepath: path to processed dataset\n",
    "            dataset_type: whether the dataset is train, dev or test dataframe\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: returns a dataframe containing processed data\n",
    "            \n",
    "        Examples:\n",
    "            >>> df = load_data('data/processed/train_set.csv')\n",
    "                df.head()\n",
    "    '''\n",
    "    filename = Path(filepath)\n",
    "    if not filename.exists():\n",
    "        raise FileNotFoundError(f'File not found! Check filepath and try again later!')\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # check that the df is not empty\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(f'Dataframe cannot be empty!')\n",
    "    \n",
    "    print(f'{dataset_type} dataframe successfully loaded with {df.shape[0]} rows and {df.shape[1]} features')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "TRAIN_DATA_PATH = '../data/processed/train_set.csv'\n",
    "DEV_DATA_PATH = '../data/processed/dev_set.csv'\n",
    "\n",
    "dev_df = load_data(DEV_DATA_PATH, 'Dev')\n",
    "train_df = load_data(TRAIN_DATA_PATH, 'Train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73832bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All validation passed!\n",
      "All validation passed!\n"
     ]
    }
   ],
   "source": [
    "# perform one last data quality check before modelling\n",
    "def data_quality_checks(df: pd.DataFrame) -> None:\n",
    "    '''\n",
    "        Performs one last data quality check before modelling\n",
    "\n",
    "        Args:\n",
    "            df: pandas' dataframe to be validated\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: if any of the validations fail\n",
    "\n",
    "        Example:\n",
    "            >>> data_quality_checks(df)\n",
    "    '''\n",
    "\n",
    "    missing = df.isnull().sum().sum()\n",
    "    if missing:\n",
    "        raise ValueError(\"The dataset must not contain null values\")\n",
    "    \n",
    "    n_duplicates = df[df.duplicated()].sum().sum()\n",
    "    if n_duplicates != 0:\n",
    "        raise ValueError(\"The dataset must not contain duplicate rows\")\n",
    "\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numerical_cols) != len(df.columns):\n",
    "        raise ValueError(\"All columns in the dataset must be numerical\")\n",
    "\n",
    "    print(\"All validation passed!\")\n",
    "\n",
    "data_quality_checks(train_df)\n",
    "data_quality_checks(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data further into features and target set\n",
    "def features_target_split(df: pd.DataFrame, target: str = 'Current_Salary') -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "        Split the dataset further into features and target splits\n",
    "        \n",
    "        This function takes the given dataframe and splits it\n",
    "        into the feature set and target set for modelling purposes\n",
    "    \n",
    "        Args:\n",
    "            df: pd.DataFrame = Provided dataframe\n",
    "            target: str = Target variable in the given dataframe\n",
    "\n",
    "        Returns:\n",
    "            A tuple of the features and target sets\n",
    "\n",
    "        Examples:\n",
    "            >>> x, y = features_target_split(df, 'Employee_salary')\n",
    "                x.head()\n",
    "                y.head()\n",
    "    '''\n",
    "    y = df[target].copy()\n",
    "    if not isinstance(y, pd.Series):\n",
    "        raise ValueError('The target variable must be a pandas series')\n",
    "    \n",
    "    if len(y) == 0:\n",
    "        raise ValueError(f'The target variable cannot be empty')\n",
    "    \n",
    "    x = df.drop(columns=[target]).copy()\n",
    "    if not isinstance(x, pd.DataFrame):\n",
    "        raise ValueError('The feature set must be a pandas dataframe')\n",
    "    \n",
    "    if len(x) == 0:\n",
    "        raise ValueError(f'The target variable cannot be empty')\n",
    "    \n",
    "    if len(y) != len(x):\n",
    "        raise ValueError('The length of the target variable must be equal to the length of the feature set')\n",
    "\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = features_target_split(train_df)\n",
    "x_dev, y_dev = features_target_split(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00989889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (35000, 43)\n",
      "x_dev: (7500, 43)\n",
      "y_train: (35000,)\n",
      "y_dev: (7500,)\n"
     ]
    }
   ],
   "source": [
    "print('x_train: ', x_train.shape)\n",
    "print(f'x_dev: {x_dev.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "print(f'y_dev: {y_dev.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213537ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3641b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, learning_curve\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,root_mean_squared_error,r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline models - performing modelling with minimal feature engineering\n",
    "models = {\n",
    "    'Ridge' : Ridge(random_state=42),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=200, max_depth=6, n_jobs=-1, random_state=1),\n",
    "    'XGBoost': XGBRegressor(),\n",
    "    'LightGBM': LGBMRegressor(random_state=2)\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f'Training model: {model_name}...')\n",
    "    start_time = time.time()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "\n",
    "    preds_train = model.predict(x_train)\n",
    "    # preds_dev = model.predict(x_dev)\n",
    "\n",
    "    results = {\n",
    "        'Model_name' : model_name,\n",
    "        'Mean_squared_error' : mean_squared_error(y_train, preds_train),\n",
    "        'Root_mean_squared_error' : root_mean_squared_error(y_train, preds_train),\n",
    "        'Mean_absolute_error' : mean_absolute_error(y_train, preds_train)\n",
    "    }\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        print(f'{name} : {result}')\n",
    "        \n",
    "    print('='*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
